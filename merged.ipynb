{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tft_data.py\n",
    "\n",
    "mergedata.py, \n",
    "\n",
    "metaGen_ordenc.py, \n",
    "\n",
    "metaList_ordenc.txt\n",
    "\n",
    "encoder.py,\n",
    "\n",
    "metaGenerator.py, (data/)\n",
    "\n",
    "model_catboost.py\n",
    "\n",
    "dataloader.py\n",
    "\n",
    "model_woo2.py\n",
    "\n",
    "Data_processing_Fixed.py \n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "\n",
    "샘플데이터(match_data1.pickle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tft api data requesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from datetime import date\n",
    "from requests.adapters import Retry, HTTPAdapter\n",
    "\n",
    "key = \"RGAPI-ff3f9cd8-0265-442b-a841-003126f2791f\" # 24시간마다 만료되므로, 재 갱신한 key를 받는 작업이 필요합니다. \n",
    "today = date.today().isoformat()\n",
    "\n",
    "\n",
    "def league_summoner(key, tier, country=\"kr\"):\n",
    "    request = requests.get(\n",
    "        f\"https://{country}.api.riotgames.com/tft/league/v1/{tier}?api_key={key}\"\n",
    "    )\n",
    "    return json.loads(request.content)\n",
    "\n",
    "\n",
    "def summoner_info(summonerId, key, n, country=\"kr\"):  # by_name\n",
    "    try:\n",
    "        request = requests.get(\n",
    "            f\"https://{country}.api.riotgames.com/tft/summoner/v1/summoners/{summonerId}?api_key={key}\",\n",
    "            timeout=5,\n",
    "        )\n",
    "    except requests.Timeout:\n",
    "        for i in range(5):\n",
    "            try:\n",
    "                print(f\"Timeout. {i+1}-th Retry...\")\n",
    "                request = requests.get(\n",
    "                    f\"https://{country}.api.riotgames.com/tft/match/v1/matches/by-puuid/{i}/ids?count={n}&api_key={key}\",\n",
    "                    timeout=5,\n",
    "                )\n",
    "                break\n",
    "            except requests.Timeout:\n",
    "                pass\n",
    "\n",
    "    if request.status_code == 429:\n",
    "        print(\"Rate limit exceeded!\")\n",
    "        sleep(120)\n",
    "        try:\n",
    "            request = requests.get(\n",
    "                f\"https://{country}.api.riotgames.com/tft/summoner/v1/summoners/{summonerId}?api_key={key}\",\n",
    "                timeout=5,\n",
    "            )\n",
    "        except requests.Timeout:\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    print(f\"Timeout. {i+1}-th Retry...\")\n",
    "                    request = requests.get(\n",
    "                        f\"https://{country}.api.riotgames.com/tft/match/v1/matches/by-puuid/{i}/ids?count={n}&api_key={key}\",\n",
    "                        timeout=5,\n",
    "                    )\n",
    "                    break\n",
    "                except requests.Timeout:\n",
    "                    pass\n",
    "\n",
    "    return json.loads(request.content)\n",
    "\n",
    "\n",
    "def summoner_info_retry(summonerId, key, country=\"kr\"):  # by_name\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=0, status_forcelist=[502, 503, 504])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    request = s.get(\n",
    "        f\"https://{country}.api.riotgames.com/tft/summoner/v1/summoners/{summonerId}?api_key={key}\"\n",
    "    )\n",
    "    return json.loads(request.content)\n",
    "\n",
    "\n",
    "def load_summonerId():\n",
    "    with open(f\"summoner_2022-11-13.pickle\", \"rb\") as f:\n",
    "        summonerId = pickle.load(f)\n",
    "    return summonerId\n",
    "\n",
    "\n",
    "def summoner_puuid(summonerId):\n",
    "    summ_puuid = pd.DataFrame()\n",
    "    step = 0\n",
    "    row_idx = 0\n",
    "    for id in summonerId:\n",
    "        step += 1\n",
    "        si = summoner_info(id, key, \"kr\")\n",
    "        si = json_normalize(si)\n",
    "        try:\n",
    "            summ_puuid = pd.concat([summ_puuid, si[\"puuid\"]], ignore_index=True)\n",
    "        except KeyError as e:\n",
    "            print(id)\n",
    "            continue\n",
    "        row_idx += 1\n",
    "        print(f\"step {step} done! : puuid = \" + si[\"puuid\"])\n",
    "\n",
    "    return summ_puuid\n",
    "\n",
    "\n",
    "def get_puuid(summonerId):\n",
    "    for i in range(0, 15):\n",
    "        print(i)\n",
    "        puuid = summoner_puuid(summonerId[i])\n",
    "        print(f\"{i}-th iter done!\")\n",
    "        with open(f\"C:/Users/edwar/TFTML/data/puuid_{i}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(puuid, f)\n",
    "\n",
    "\n",
    "def get_matchid(puuid, key, n, region=\"asia\"):\n",
    "    matchid = []\n",
    "    step = 0\n",
    "    for i in puuid:\n",
    "        step += 1\n",
    "        try:\n",
    "            request = requests.get(\n",
    "                f\"https://{region}.api.riotgames.com/tft/match/v1/matches/by-puuid/{i}/ids?count={n}&api_key={key}\",\n",
    "                timeout=5,\n",
    "            )\n",
    "        except requests.Timeout:\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    print(f\"Timeout. {i+1}-th Retry...\")\n",
    "                    request = requests.get(\n",
    "                        f\"https://{region}.api.riotgames.com/tft/match/v1/matches/by-puuid/{i}/ids?count={n}&api_key={key}\",\n",
    "                        timeout=5,\n",
    "                    )\n",
    "                    break\n",
    "                except requests.Timeout:\n",
    "                    pass\n",
    "\n",
    "        if request.status_code == 429:\n",
    "            print(\"Rate limit exceeded!\")\n",
    "            sleep(120)\n",
    "            try:\n",
    "                request = requests.get(\n",
    "                    f\"https://{region}.api.riotgames.com/tft/match/v1/matches/by-puuid/{i}/ids?count={n}&api_key={key}\",\n",
    "                    timeout=5,\n",
    "                )\n",
    "            except requests.Timeout:\n",
    "                request = requests.get(\n",
    "                    f\"https://{region}.api.riotgames.com/tft/match/v1/matches/by-puuid/{i}/ids?count={n}&api_key={key}\",\n",
    "                    timeout=5,\n",
    "                )\n",
    "        request = json.loads(request.content)\n",
    "        print(request)\n",
    "        matchid.extend(request)\n",
    "        print(f\"step {step} done!\")\n",
    "    return list(set(matchid))\n",
    "\n",
    "\n",
    "def match_info(matchid, key, region=\"asia\"):\n",
    "    game_record = pd.DataFrame()\n",
    "    step = 0\n",
    "    for i in matchid:\n",
    "        step += 1\n",
    "        try:\n",
    "            request = requests.get(\n",
    "                f\"https://{region}.api.riotgames.com/tft/match/v1/matches/{i}?api_key={key}\",\n",
    "                timeout=5,\n",
    "            )\n",
    "        except requests.Timeout:\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    print(f\"Timeout. {i+1}-th Retry...\")\n",
    "                    request = requests.get(\n",
    "                        f\"https://{region}.api.riotgames.com/tft/match/v1/matches/{i}?api_key={key}\",\n",
    "                        timeout=5,\n",
    "                    )\n",
    "                    break\n",
    "                except requests.Timeout:\n",
    "                    pass\n",
    "\n",
    "        if request.status_code == 429:\n",
    "            print(\"Rate limit exceeded!\")\n",
    "            sleep(120)\n",
    "            try:\n",
    "                request = requests.get(\n",
    "                    f\"https://{region}.api.riotgames.com/tft/match/v1/matches/{i}?api_key={key}\",\n",
    "                    timeout=5,\n",
    "                )\n",
    "            except requests.Timeout:\n",
    "                for i in range(5):\n",
    "                    try:\n",
    "                        print(f\"Timeout. {i+1}-th Retry...\")\n",
    "                        request = requests.get(\n",
    "                            f\"https://{region}.api.riotgames.com/tft/match/v1/matches/{i}?api_key={key}\",\n",
    "                            timeout=5,\n",
    "                        )\n",
    "                    except requests.Timeout:\n",
    "                        pass\n",
    "        request = json.loads(request.content)\n",
    "        request = json_normalize(request)\n",
    "        print(request)\n",
    "        game_record = pd.concat([game_record, request])\n",
    "        print(f\"step {step} done!\")\n",
    "    return game_record\n",
    "\n",
    "\n",
    "\"\"\" # summonerId -> puuid -> matchId 받아오는 과정. 이미 받아왔으므로 주석처리했습니다. 학습에선 속도의 용이를 위해 pickle파일로 저장하여 사용했습니다.\n",
    "summonerId = load_summonerId()\n",
    "print(summonerId)\n",
    "\n",
    "ch_matchid_list = []\n",
    "for i in range(0, 15):\n",
    "    with open(f\"data/puuid_{i}.pickle\", \"rb\") as f:\n",
    "        puuid = pickle.load(f)\n",
    "    puuid_list = puuid.values.tolist()\n",
    "    puuid_list = [i[0] for i in puuid_list]\n",
    "    ch_matchid = get_matchid(puuid_list, key, 30)\n",
    "    print(f\"{i}-th iter done!\")\n",
    "    with open(f\"data/matchid_{i}.pickle\", \"wb\") as f:\n",
    "        pickle.dump(ch_matchid, f)\n",
    "\"\"\"\n",
    "\n",
    "for i in range(22, 31):\n",
    "    with open(\n",
    "        f\"data/merged_match_id/match_id{i}.pickle\", \"rb\"\n",
    "    ) as f:\n",
    "        matchid = pickle.load(f)\n",
    "    match_data = match_info(matchid, key, \"asia\")\n",
    "    with open(f\"data/match_data/match_data{i}.pickle\", \"wb\") as f:\n",
    "        pickle.dump(match_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import pickle\n",
    "\n",
    "data = pd.DataFrame()\n",
    "# 1~7\n",
    "for i in range(1, 8):\n",
    "    with open(f\"data/match_data/match_data{i}.pickle\", \"rb\") as f:\n",
    "        data_t = pickle.load(f)\n",
    "    data = pd.concat([data, data_t])\n",
    "\n",
    "# 9~15\n",
    "for i in range(9, 16):\n",
    "    with open(f\"data/match_data/match_data{i}.pickle\", \"rb\") as f:\n",
    "        data_t = pickle.load(f)\n",
    "    data = pd.concat([data, data_t])\n",
    "\n",
    "# 22~30\n",
    "for i in range(22, 31):\n",
    "    with open(f\"data/match_data/match_data{i}.pickle\", \"rb\") as f:\n",
    "        data_t = pickle.load(f)\n",
    "    data = pd.concat([data, data_t])\n",
    "\n",
    "# with open(\"data/match_data/match_info20.pickle\", \"rb\") as f:\n",
    "#     data_t = pickle.load(f)\n",
    "\n",
    "data = pd.concat([data, data_t])\n",
    "\n",
    "with open(f\"data/match_data/match_data.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "# data.to_csv(\"data/match_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "print(len(data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "f = open(\"data/check.csv\", \"r\")\n",
    "\n",
    "# f = open(\"data/match_data/temp_ch_2022-11-10.csv\", \"r\")\n",
    "# read data from pickle file\n",
    "df = pd.read_pickle(\"data/match_data/match_data22.pickle\")\n",
    "\n",
    "MetaData_item = [\n",
    "    \"TFT_Item_ArchangelsStaff\",\n",
    "    \"TFT_Item_Bloodthirster\",\n",
    "    \"TFT_Item_BrambleVest\",\n",
    "    \"TFT_Item_Chalice\",\n",
    "    \"TFT_Item_Deathblade\",\n",
    "    \"TFT_Item_DragonsClaw\",\n",
    "    \"TFT_Item_ForceOfNature\",\n",
    "    \"TFT_Item_FrozenHeart\",\n",
    "    \"TFT_Item_GargoyleStoneplate\",\n",
    "    \"TFT_Item_GuardianAngel\",\n",
    "    \"TFT_Item_GuinsoosRageblade\",\n",
    "    \"TFT_Item_HextechGunblade\",\n",
    "    \"TFT_Item_InfinityEdge\",\n",
    "    \"TFT_Item_IonicSpark\",\n",
    "    \"TFT_Item_JeweledGauntlet\",\n",
    "    \"TFT_Item_LastWhisper\",\n",
    "    \"TFT_Item_LocketOfTheIronSolari\",\n",
    "    \"TFT_Item_MadredsBloodrazor\",\n",
    "    \"TFT_Item_Morellonomicon\",\n",
    "    \"TFT_Item_PowerGauntlet\",\n",
    "    \"TFT_Item_Quicksilver\",\n",
    "    \"TFT_Item_RabadonsDeathcap\",\n",
    "    \"TFT_Item_RapidFireCannon\",\n",
    "    \"TFT_Item_RedBuff\",\n",
    "    \"TFT_Item_Redemption\",\n",
    "    \"TFT_Item_RunaansHurricane\",\n",
    "    \"TFT_Item_SeraphsEmbrace\",\n",
    "    \"TFT_Item_Shroud\",\n",
    "    \"TFT_Item_SpearOfShojin\",\n",
    "    \"TFT_Item_StatikkShiv\",\n",
    "    \"TFT_Item_ThiefsGloves\",\n",
    "    \"TFT_Item_TitanicHydra\",\n",
    "    \"TFT_Item_TitansResolve\",\n",
    "    \"TFT_Item_UnstableConcoction\",\n",
    "    \"TFT_Item_WarmogsArmor\",\n",
    "    \"TFT_Item_ZekesHerald\",\n",
    "    \"TFT_Item_Zephyr\",\n",
    "]\n",
    "\n",
    "X_ch_add = []\n",
    "Y_ch = []\n",
    "\n",
    "Combination_Item = [\n",
    "    \"TFT_Item_BFSword\",\n",
    "    \"TFT_Item_ChainVest\",\n",
    "    \"TFT_Item_GiantsBelt\",\n",
    "    \"TFT_Item_NeedlesslyLargeRod\",\n",
    "    \"TFT_Item_NegatronCloak\",\n",
    "    \"TFT_Item_RecurveBow\",\n",
    "    \"TFT_Item_SparringGloves\",\n",
    "    \"TFT_Item_Spatula\",\n",
    "    \"TFT_Item_TearOfTheGoddess\",\n",
    "]\n",
    "\n",
    "# delete 404 Not Found and pairs & turbo mode\n",
    "df = df[df[\"status.status_code\"].isna()]\n",
    "df = df[df[\"info.tft_game_type\"] == \"standard\"]\n",
    "print(len(df))\n",
    "\n",
    "for game in df[\"info.participants\"]:\n",
    "    # game is list of 8 players\n",
    "    # j has augments, companion, gold_left, last_round, level, placement, players_eliminated, puuid, traits, units\n",
    "    # j is single player\n",
    "    for j in game:\n",
    "        X_ch = []\n",
    "        traits = []\n",
    "        items = []\n",
    "        # trait style is activated number of synergy (e.g. 석호6)\n",
    "        for trait in j[\"traits\"]:\n",
    "            traits.append(trait[\"name\"] + str(trait[\"style\"]))\n",
    "        for champ in j[\"units\"]:\n",
    "            if champ[\"itemNames\"] != []:\n",
    "                for item in champ[\"itemNames\"]:\n",
    "                    if item not in MetaData_item:\n",
    "                        del item\n",
    "                if champ[\"itemNames\"][0] == \"TFT_Item_ThiefsGloves\":\n",
    "                    items.append(champ[\"itemNames\"][0])\n",
    "                else:\n",
    "                    for item in champ[\"itemNames\"]:\n",
    "                        items.append(item)\n",
    "\n",
    "        X_ch.extend(j[\"augments\"])  # append -> extend\n",
    "        X_ch.extend(traits)\n",
    "        X_ch.extend(items)\n",
    "        Y_ch.append(j[\"placement\"])\n",
    "        X_ch_add.append(X_ch)\n",
    "\n",
    "# print(X_ch_add)\n",
    "# print(Y_ch)\n",
    "\n",
    "print(len(X_ch_add))\n",
    "print(len(X_ch_add[4]))\n",
    "print(X_ch_add[0])\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metaGen_ordenc.py -> metaList_ordenc생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta generator for ordered hot encoding\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"data/match_data/match_data.pickle\")\n",
    "\n",
    "item_set = set()\n",
    "augment_set = set()\n",
    "trait_set = set()\n",
    "\n",
    "Combination_Item = [\n",
    "    \"TFT_Item_B.F.Sword\",\n",
    "    \"TFT_Item_BFSword\",\n",
    "    \"TFT_Item_ChainVest\",\n",
    "    \"TFT_Item_GiantsBelt\",\n",
    "    \"TFT_Item_NeedlesslyLargeRod\",\n",
    "    \"TFT_Item_NegatronCloak\",\n",
    "    \"TFT_Item_RecurveBow\",\n",
    "    \"TFT_Item_SparringGloves\",\n",
    "    \"TFT_Item_Spatula\",\n",
    "    \"TFT_Item_TearOfTheGoddess\",\n",
    "]\n",
    "\n",
    "df = df[df[\"status.status_code\"].isna()]\n",
    "df = df[df[\"info.tft_game_type\"] == \"standard\"]\n",
    "\n",
    "for game in df[\"info.participants\"]:\n",
    "    for j in game:\n",
    "        # trait\n",
    "        for trait in j[\"traits\"]:\n",
    "            trait_set.add(trait[\"name\"])\n",
    "\n",
    "        # item\n",
    "        for item in j[\"units\"]:\n",
    "            if item[\"itemNames\"] != []:\n",
    "                item_list = item[\"itemNames\"]\n",
    "                if \"TFT_Item_ThiefsGloves\" in item_list:\n",
    "                    item_set.add(\"TFT_Item_ThiefsGloves\")\n",
    "                    break\n",
    "\n",
    "                for item in item_list:\n",
    "                    if item not in Combination_Item:\n",
    "                        item_set.add(item)\n",
    "\n",
    "        # augment\n",
    "        for aug in j[\"augments\"]:\n",
    "            augment_set.add(aug)\n",
    "\n",
    "\n",
    "print(\"[item set]\\n\")\n",
    "print(item_set)\n",
    "\n",
    "print(\"[trait set]\\n\")\n",
    "print(trait_set)\n",
    "\n",
    "print(\"[augment set]\\n\")\n",
    "print(augment_set)\n",
    "\n",
    "f = open(\"metaList_ordenc.txt\", \"w\")\n",
    "for i in sorted(item_set):\n",
    "    if not i.startswith(\"TFT_Tutorial\"):\n",
    "        f.write(i + \"\\n\")\n",
    "for i in sorted(trait_set):\n",
    "    if not i.startswith(\"TFT_Tutorial\"):\n",
    "        f.write(i + \"\\n\")\n",
    "for i in sorted(augment_set):\n",
    "    if not i.startswith(\"TFT_Tutorial\"):\n",
    "        f.write(i + \"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta Generator -> metaList4.txt생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# f = open(\"./match_data/temp_ch_2022-11-10.csv\", \"r\")\n",
    "# f = open(\"./check.csv\", \"r\")\n",
    "df = pd.read_pickle(\"data/match_data/match_data.pickle\")\n",
    "\n",
    "item_set = set()\n",
    "augment_set = set()\n",
    "trait_set = set()\n",
    "\n",
    "Combination_Item = [\n",
    "    \"TFT_Item_B.F.Sword\",\n",
    "    \"TFT_Item_BFSword\",\n",
    "    \"TFT_Item_ChainVest\",\n",
    "    \"TFT_Item_GiantsBelt\",\n",
    "    \"TFT_Item_NeedlesslyLargeRod\",\n",
    "    \"TFT_Item_NegatronCloak\",\n",
    "    \"TFT_Item_RecurveBow\",\n",
    "    \"TFT_Item_SparringGloves\",\n",
    "    \"TFT_Item_Spatula\",\n",
    "    \"TFT_Item_TearOfTheGoddess\",\n",
    "]\n",
    "\n",
    "df = df[df[\"status.status_code\"].isna()]\n",
    "df = df[df[\"info.tft_game_type\"] == \"standard\"]\n",
    "\n",
    "for game in df[\"info.participants\"]:\n",
    "    for j in game:\n",
    "        # trait\n",
    "        for trait in j[\"traits\"]:\n",
    "            trait_set.add(trait[\"name\"] + str(trait[\"tier_current\"]))\n",
    "\n",
    "        # item\n",
    "        for item in j[\"units\"]:\n",
    "            if item[\"itemNames\"] != []:\n",
    "                item_list = item[\"itemNames\"]\n",
    "                if \"TFT_Item_ThiefsGloves\" in item_list:\n",
    "                    item_set.add(\"TFT_Item_ThiefsGloves\")\n",
    "                    break\n",
    "\n",
    "                for item in item_list:\n",
    "                    if item not in Combination_Item:\n",
    "                        item_set.add(item)\n",
    "\n",
    "        # augment\n",
    "        for aug in j[\"augments\"]:\n",
    "            augment_set.add(aug)\n",
    "\n",
    "\"\"\"\n",
    "print(\"[item set]\\n\")\n",
    "pprint(item_set)\n",
    "\n",
    "print(\"[trait set]\\n\")\n",
    "pprint(trait_set)\n",
    "\n",
    "print(\"[augment set]\\n\")\n",
    "pprint(augment_set)\n",
    "\"\"\"\n",
    "f = open(\"metaList4.txt\", \"w\")\n",
    "for i in sorted(item_set):\n",
    "    if not i.startswith(\"TFT_Tutorial\"):\n",
    "        f.write(i + \"\\n\")\n",
    "for i in sorted(trait_set):\n",
    "    if not i.startswith(\"TFT_Tutorial\"):\n",
    "        f.write(i + \"\\n\")\n",
    "for i in sorted(augment_set):\n",
    "    f.write(i + \"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "'TFTTutorial_Demon0',\n",
    " 'TFTTutorial_Gunslinger0',\n",
    " 'TFTTutorial_Imperial0',\n",
    " 'TFTTutorial_Pirate0',\n",
    " 'TFTTutorial_Ranger0',\n",
    " 'TFTTutorial_Shapeshifter0',\n",
    " 'TFTTutorial_Sorcerer0',\n",
    " 'TFTTutorial_TutorialPrimary1',\n",
    " 'TFTTutorial_TutorialSecondary1' \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    " 'TFT4_Item_OrnnAnimaVisage',\n",
    " 'TFT4_Item_OrnnDeathsDefiance',\n",
    " 'TFT4_Item_OrnnEternalWinter',\n",
    " 'TFT4_Item_OrnnInfinityForce',\n",
    " 'TFT4_Item_OrnnMuramana',\n",
    " 'TFT4_Item_OrnnObsidianCleaver',\n",
    " 'TFT4_Item_OrnnRanduinsSanctum',\n",
    " 'TFT4_Item_OrnnRocketPropelledFist',\n",
    " 'TFT4_Item_OrnnTheCollector',\n",
    " 'TFT4_Item_OrnnZhonyasParadox',\n",
    " 'TFT5_Item_ArchangelsStaffRadiant',\n",
    " 'TFT5_Item_BloodthirsterRadiant',\n",
    " 'TFT5_Item_BlueBuffRadiant',\n",
    " 'TFT5_Item_BrambleVestRadiant',\n",
    " 'TFT5_Item_ChaliceOfPowerRadiant',\n",
    " 'TFT5_Item_DeathbladeRadiant',\n",
    " 'TFT5_Item_DragonsClawRadiant',\n",
    " 'TFT5_Item_FrozenHeartRadiant',\n",
    " 'TFT5_Item_GargoyleStoneplateRadiant',\n",
    " 'TFT5_Item_GiantSlayerRadiant',\n",
    " 'TFT5_Item_GuardianAngelRadiant',\n",
    " 'TFT5_Item_GuinsoosRagebladeRadiant',\n",
    " 'TFT5_Item_HandOfJusticeRadiant',\n",
    " 'TFT5_Item_HextechGunbladeRadiant',\n",
    " 'TFT5_Item_InfinityEdgeRadiant',\n",
    " 'TFT5_Item_IonicSparkRadiant',\n",
    " 'TFT5_Item_JeweledGauntletRadiant',\n",
    " 'TFT5_Item_LastWhisperRadiant',\n",
    " 'TFT5_Item_LocketOfTheIronSolariRadiant',\n",
    " 'TFT5_Item_MorellonomiconRadiant',\n",
    " 'TFT5_Item_QuicksilverRadiant',\n",
    " 'TFT5_Item_RabadonsDeathcapRadiant',\n",
    " 'TFT5_Item_RapidFirecannonRadiant',\n",
    " 'TFT5_Item_RedemptionRadiant',\n",
    " 'TFT5_Item_RunaansHurricaneRadiant',\n",
    " 'TFT5_Item_ShroudOfStillnessRadiant',\n",
    " 'TFT5_Item_SpearOfShojinRadiant',\n",
    " 'TFT5_Item_StatikkShivRadiant',\n",
    " 'TFT5_Item_SunfireCapeRadiant',\n",
    " 'TFT5_Item_ThiefsGlovesRadiant',\n",
    " 'TFT5_Item_TitansResolveRadiant',\n",
    " 'TFT5_Item_TrapClawRadiant',\n",
    " 'TFT5_Item_WarmogsArmorRadiant',\n",
    " 'TFT5_Item_ZekesHeraldRadiant',\n",
    " 'TFT5_Item_ZephyrRadiant',\n",
    " 'TFT5_Item_ZzRotPortalRadiant',\n",
    " 'TFT7_Item_AssassinEmblemItem',\n",
    " 'TFT7_Item_BruiserEmblemItem',\n",
    " 'TFT7_Item_CannoneerEmblemItem',\n",
    " 'TFT7_Item_CavalierEmblemItem',\n",
    " 'TFT7_Item_DarkflightEmblemItem',\n",
    " 'TFT7_Item_DarkflightEssence',\n",
    " 'TFT7_Item_DragonmancerEmblemItem',\n",
    " 'TFT7_Item_EvokerEmblemItem',\n",
    " 'TFT7_Item_GuardianEmblemItem',\n",
    " 'TFT7_Item_GuildEmblemItem',\n",
    " 'TFT7_Item_JadeEmblemItem',\n",
    " 'TFT7_Item_LagoonEmblemItem',\n",
    " 'TFT7_Item_MageEmblemItem',\n",
    " 'TFT7_Item_MirageEmblemItem',\n",
    " 'TFT7_Item_MysticEmblemItem',\n",
    " 'TFT7_Item_ScalescornEmblemItem',\n",
    " 'TFT7_Item_ShimmerscaleCrownOfChampions',\n",
    " 'TFT7_Item_ShimmerscaleDeterminedInvestor',\n",
    " 'TFT7_Item_ShimmerscaleDeterminedInvestor_HR',\n",
    " 'TFT7_Item_ShimmerscaleDiamondHands',\n",
    " 'TFT7_Item_ShimmerscaleDiamondHands_HR',\n",
    " 'TFT7_Item_ShimmerscaleDravensAxe',\n",
    " 'TFT7_Item_ShimmerscaleDravensAxe_HR',\n",
    " 'TFT7_Item_ShimmerscaleEmblemItem',\n",
    " 'TFT7_Item_ShimmerscaleGamblersBlade',\n",
    " 'TFT7_Item_ShimmerscaleGamblersBlade_HR',\n",
    " 'TFT7_Item_ShimmerscaleGoldmancersStaff',\n",
    " 'TFT7_Item_ShimmerscaleGoldmancersStaff_HR',\n",
    " 'TFT7_Item_ShimmerscaleHeartOfGold',\n",
    " 'TFT7_Item_ShimmerscaleMogulsMail',\n",
    " 'TFT7_Item_ShimmerscaleMogulsMail_HR',\n",
    " 'TFT7_Item_SwiftshotEmblemItem',\n",
    " 'TFT7_Item_TempestEmblemItem',\n",
    " 'TFT7_Item_WarriorEmblemItem'\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import category_encoders as ce\n",
    "import math\n",
    "\n",
    "# rarity - cost array\n",
    "# e.g. Ao Shin is Rarity 7, 10 cost\n",
    "cost = [0, 1, 2, 3, 4, 8, 5, 10]\n",
    "pw = [\n",
    "    [0, 1, 2, 3, 4, 8, 5, 10],\n",
    "    [\n",
    "        0,\n",
    "        3,\n",
    "        5,\n",
    "    ],\n",
    "]\n",
    "\n",
    "\n",
    "def multi_hot_encoding(df):\n",
    "    ## Feature list import\n",
    "    MetaData = []\n",
    "    # open metadata for multi hot encoding\n",
    "    with open(\"./data/metaList4.txt\", \"r\") as a:\n",
    "        s = a.readlines()\n",
    "    for line in s:\n",
    "        res = re.sub(\"'|,| |\\n\", \"\", line)\n",
    "        MetaData.append(res)\n",
    "\n",
    "    X = []\n",
    "    raw_X = []\n",
    "    label = []\n",
    "    for game in df[\"info.participants\"]:\n",
    "        # game is list of 8 players\n",
    "        # j has augments, companion, gold_left, last_round, level, placement, players_eliminated, puuid, traits, units\n",
    "        # j is single player\n",
    "        for j in game:\n",
    "            value = 0\n",
    "            X_i = []\n",
    "            # ignore if died before 4-2 (outlier)\n",
    "            if len(j[\"augments\"]) < 3:\n",
    "                continue\n",
    "            # trait tier_current is activated number of synergy (e.g. 석호6)\n",
    "            for trait in j[\"traits\"]:\n",
    "                if trait[\"name\"] + str(trait[\"tier_current\"]) in MetaData:\n",
    "                    X_i.append(trait[\"name\"] + str(trait[\"num_units\"]))\n",
    "            for champ in j[\"units\"]:\n",
    "                value += cost[champ[\"rarity\"]] * pow(3, champ[\"tier\"] - 1)\n",
    "                if champ[\"itemNames\"] != []:\n",
    "                    if champ[\"itemNames\"][0] == \"TFT_Item_ThiefsGloves\":\n",
    "                        X_i.append(\"TFT_Item_ThiefsGloves\")\n",
    "                    else:\n",
    "                        for item in champ[\"itemNames\"]:\n",
    "                            if item in MetaData:\n",
    "                                X_i.append(item)\n",
    "            for aug in j[\"augments\"]:\n",
    "                if aug in MetaData:\n",
    "                    X_i.append(aug)\n",
    "\n",
    "            X_i.append(value)  # total cost of deck\n",
    "            X_i.append(j[\"total_damage_to_players\"])  # total damage to players\n",
    "            raw_X.append(j)\n",
    "            label.append(j[\"placement\"])  # rank (label)\n",
    "            X.append(sorted(X_i))  # sorted\n",
    "\n",
    "    X = [[1 if i in row_x else 0 for i in MetaData] for row_x in X]\n",
    "    # multi-hot_encoding\n",
    "    return X, label, raw_X\n",
    "\n",
    "\n",
    "def ordered_target_encoding(df):\n",
    "    ## Feature list import\n",
    "    MetaData = []\n",
    "    with open(\"./data/metaList_ordenc.txt\", \"r\") as a:\n",
    "        s = a.readlines()\n",
    "    for line in s:\n",
    "        res = re.sub(\"'|,| |\\n\", \"\", line)\n",
    "        MetaData.append(res)\n",
    "\n",
    "    # print(MetaData)\n",
    "    X = []\n",
    "    raw_X = []\n",
    "    label = []\n",
    "    train_aug1 = []\n",
    "    train_aug2 = []\n",
    "    train_aug3 = []\n",
    "    train_aug = {}\n",
    "    # depenency category_encoders\n",
    "    cbe_aug = ce.CatBoostEncoder()\n",
    "    for game in df[\"info.participants\"]:\n",
    "        # game is list of 8 players\n",
    "        # j has augments, companion, gold_left, last_round, level, placement, players_eliminated, puuid, traits, units\n",
    "        # j is single player\n",
    "        for j in game:\n",
    "            # item (112) + trait (29) + aug (3) + special feature (2) = 146\n",
    "            value = 0\n",
    "            X_i = [0 for i in range(141)]\n",
    "            # ignore if died before 4-2 (outlier)\n",
    "            if len(j[\"augments\"]) < 3:\n",
    "                continue\n",
    "            for champ in j[\"units\"]:\n",
    "                value += cost[champ[\"rarity\"]] * pow(3, champ[\"tier\"] - 1)\n",
    "                if champ[\"itemNames\"] != []:\n",
    "                    if champ[\"itemNames\"][0] == \"TFT_Item_ThiefsGloves\":\n",
    "                        idx = MetaData.index(\"TFT_Item_ThiefsGloves\")\n",
    "                        X_i[idx] = 1\n",
    "                    else:\n",
    "                        for item in champ[\"itemNames\"]:\n",
    "                            if item in MetaData:\n",
    "                                idx = MetaData.index(item)\n",
    "                                X_i[idx] = 1\n",
    "\n",
    "            # trait tier_current is activated number of synergy (e.g. 석호6)\n",
    "            for trait in j[\"traits\"]:\n",
    "                if trait[\"name\"] in MetaData:\n",
    "                    idx = MetaData.index(trait[\"name\"])\n",
    "                    # print(\"trait index is \" + str(idx))\n",
    "                    # print(trait[\"tier_current\"])\n",
    "                    X_i[idx] = math.exp(trait[\"num_units\"])\n",
    "            train_aug1.append(j[\"augments\"][0])\n",
    "            train_aug2.append(j[\"augments\"][1])\n",
    "            train_aug3.append(j[\"augments\"][2])\n",
    "            X_i.append(value)  # total cost of deck\n",
    "            X_i.append(j[\"total_damage_to_players\"])  # total damage to players\n",
    "            label.append(j[\"placement\"])  # rank (label)\n",
    "            X.append(X_i)\n",
    "            raw_X.append(j)\n",
    "\n",
    "    train_aug[\"aug1\"] = train_aug1\n",
    "    train_aug[\"aug2\"] = train_aug2\n",
    "    train_aug[\"aug3\"] = train_aug3\n",
    "    print(\"Start cbe\")\n",
    "    print(len(train_aug1), len(train_aug2), len(train_aug3), len(label))\n",
    "    train_aug_df = pd.DataFrame(train_aug)\n",
    "\n",
    "    # X of cbe_aug should be pd.DataFrame\n",
    "    # Ordered Target Encoding\n",
    "    cbe_aug.fit(train_aug_df, label)\n",
    "    aug_cbe = cbe_aug.transform(train_aug_df)\n",
    "    for i in range(len(X)):\n",
    "        # print(aug_cbe[\"aug1\"][i], aug_cbe[\"aug2\"][i], aug_cbe[\"aug3\"][i])\n",
    "        X[i].append(aug_cbe[\"aug1\"][i])\n",
    "        X[i].append(aug_cbe[\"aug2\"][i])\n",
    "        X[i].append(aug_cbe[\"aug3\"][i])\n",
    "        assert len(X[i]) == 146\n",
    "    return X, label, raw_X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import category_encoders as ce\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, transform=None):\n",
    "    #   데이터셋의 전처리를 해주는 부분\n",
    "        self.transform = transform\n",
    "        ## Feature list import\n",
    "        df = pd.read_pickle(\"./data/match_data/match_data1.pickle\")\n",
    "        # delete 404 and pairs & turbo mode\n",
    "        df = df[df[\"status.status_code\"].isna()]\n",
    "        df = df[df[\"info.tft_game_type\"] == \"standard\"]\n",
    "        \n",
    "        MetaData = []\n",
    "        cost = [0, 1, 2, 3, 4, 8, 5, 10]\n",
    "        with open(\"./data/metaList_ordenc.txt\", \"r\") as a:\n",
    "            s = a.readlines()\n",
    "        for line in s:\n",
    "            res = re.sub(\"'|,| |\\n\", \"\", line)\n",
    "            MetaData.append(res)\n",
    "\n",
    "        # print(MetaData)\n",
    "        self.X = []\n",
    "        raw_X = []\n",
    "        self.label = []\n",
    "        train_aug1 = []\n",
    "        train_aug2 = []\n",
    "        train_aug3 = []\n",
    "        train_aug = {}\n",
    "        # depenency category_encoders\n",
    "        cbe_aug = ce.CatBoostEncoder()\n",
    "        for game in df[\"info.participants\"]:\n",
    "            # game is list of 8 players\n",
    "            # j has augments, companion, gold_left, last_round, level, placement, players_eliminated, puuid, traits, units\n",
    "            # j is single player\n",
    "            for j in game:\n",
    "                # item (112) + trait (29) + aug (3) + value (2) = 146\n",
    "                value = 0\n",
    "                X_i = [0 for i in range(141)]\n",
    "                # ignore if died before 4-2\n",
    "                if len(j[\"augments\"]) < 3:\n",
    "                    continue\n",
    "                for champ in j[\"units\"]:\n",
    "                    value += cost[champ[\"rarity\"]] * pow(3, champ[\"tier\"] - 1)\n",
    "                    if champ[\"itemNames\"] != []:\n",
    "                        if champ[\"itemNames\"][0] == \"TFT_Item_ThiefsGloves\":\n",
    "                            idx = MetaData.index(\"TFT_Item_ThiefsGloves\")\n",
    "                            X_i[idx] = 1\n",
    "                        else:\n",
    "                            for item in champ[\"itemNames\"]:\n",
    "                                if item in MetaData:\n",
    "                                    idx = MetaData.index(item)\n",
    "                                    X_i[idx] = 1\n",
    "\n",
    "                # trait tier_current is activated number of synergy (e.g. 석호6)\n",
    "                for trait in j[\"traits\"]:\n",
    "                    if trait[\"name\"] in MetaData:\n",
    "                        idx = MetaData.index(trait[\"name\"])\n",
    "                        # print(\"trait index is \" + str(idx))\n",
    "                        # print(trait[\"tier_current\"])\n",
    "                        X_i[idx] = math.exp(trait[\"num_units\"]) / 4\n",
    "                train_aug1.append(j[\"augments\"][0])\n",
    "                train_aug2.append(j[\"augments\"][1])\n",
    "                train_aug3.append(j[\"augments\"][2])\n",
    "                X_i.append(value)  # total cost of deck\n",
    "                X_i.append(j[\"total_damage_to_players\"])  # total damage to players\n",
    "                self.label.append(j[\"placement\"])\n",
    "                self.X.append(X_i)\n",
    "                raw_X.append(j)\n",
    "\n",
    "        train_aug[\"aug1\"] = train_aug1\n",
    "        train_aug[\"aug2\"] = train_aug2\n",
    "        train_aug[\"aug3\"] = train_aug3\n",
    "        # print(\"Start cbe\")\n",
    "        # print(len(train_aug1), len(train_aug2), len(train_aug3), len(label))\n",
    "        train_aug_df = pd.DataFrame(train_aug)\n",
    "\n",
    "        # X of cbe_aug should be pd.DataFrame\n",
    "        cbe_aug.fit(train_aug_df, self.label)\n",
    "        aug_cbe = cbe_aug.transform(train_aug_df)\n",
    "        for i in range(len(self.X)):\n",
    "            # print(aug_cbe[\"aug1\"][i], aug_cbe[\"aug2\"][i], aug_cbe[\"aug3\"][i])\n",
    "            self.X[i].append(aug_cbe[\"aug1\"][i])\n",
    "            self.X[i].append(aug_cbe[\"aug2\"][i])\n",
    "            self.X[i].append(aug_cbe[\"aug3\"][i])\n",
    "            assert len(self.X[i]) == 146\n",
    "        # return X, label, raw_X\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #   데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "        return len(self.X)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        X_ = self.X[idx]\n",
    "        if self.transform:\n",
    "            X_ = self.transform(X_)\n",
    "        #   데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "        # X_ = np.expand_dims(X_, axis=(1,2)) # just DNN에선 지우기\n",
    "        X_ = torch.tensor(X_).to(torch.float32)\n",
    "        # X_ = X_.expand(-1, 4, 4)\n",
    "        # (146, 1, 1) -> (146, 4, 4) !!!!! 인코딩 값 146 시너지 + 아이템 + 증강체 + 커스텀 밸류 \n",
    "        # X_ = X_.expand(-1, 8, 8)\n",
    "        # X_ = X_.expand(-1, 16, 16)\n",
    "        \n",
    "        label_ = torch.tensor(self.label[idx]-1).to(torch.float32)\n",
    "        # label_ = label_.view(1)\n",
    "        label_ = label_.unsqueeze(0)\n",
    "        # print(label_.shape,\"Wdqw\")\n",
    "        # print(X_.shape)\n",
    "        # print(\"asdasd\", X_i.shape)\n",
    "        \n",
    "        # print(\"asdasd\", X_i.shape)\n",
    "        return X_, label_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X):\n",
    "    l = len(X)\n",
    "    train = X[0 : l // 2]\n",
    "    valid = X[l // 2 : l * 3 // 4]\n",
    "    test = X[l * 3 // 4 : l]\n",
    "    assert l == len(train) + len(valid) + len(test)\n",
    "    return train, valid, test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://catboost.ai/en/docs/concepts/python-reference_catboost\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
    "import pandas as pd\n",
    "from encoder import ordered_target_encoding, multi_hot_encoding\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "time = datetime.now(pytz.timezone(\"Asia/Seoul\")).strftime(\"%Y%m%d_%H%M%S\")\n",
    "# read data from merged pickle\n",
    "# df = pd.read_pickle(\"./data/match_data/match_data.pickle\") \n",
    "# 이 파일(match_data.pickle)이 match_data1~30의 내용이 모두 merge된 파일이며, 본 러닝에서 사용했습니다. 그러나 용량 문제로 데모 데이터셋(match_data1.pickle)을 대신 제출했습니다. \n",
    "df = pd.read_pickle(\"./data/match_data/match_data1.pickle\")\n",
    "\n",
    "# delete 404 Not Found and pairs & turbo mode\n",
    "df = df[df[\"status.status_code\"].isna()]\n",
    "df = df[df[\"info.tft_game_type\"] == \"standard\"]\n",
    "print(len(df))\n",
    "\n",
    "# get ordered target encoded categorical features\n",
    "X, label, X_raw = ordered_target_encoding(df)\n",
    "assert len(X) == len(label)\n",
    "\n",
    "# split dataset into train, valid, test\n",
    "train_X, valid_X, test_X = split_data(X)\n",
    "train_label, valid_label, test_label = split_data(label)\n",
    "train_X_raw, valid_X_raw, test_X_raw = split_data(X_raw)\n",
    "\n",
    "\n",
    "train_data = Pool(data=train_X, label=train_label)\n",
    "valid_data = Pool(data=valid_X, label=valid_label)\n",
    "test_data = Pool(data=test_X, label=test_label)\n",
    "# default lr = 0.03\n",
    "# CatBoost Regression Tree\n",
    "model = CatBoostRegressor(iterations=5000, task_type=\"GPU\")\n",
    "\n",
    "print(\"Train Start!\")\n",
    "\n",
    "# Train\n",
    "model.fit(train_data, eval_set=valid_data, plot=True)\n",
    "print(model.get_best_iteration())\n",
    "print(model.get_best_score())\n",
    "\n",
    "# Inference\n",
    "preds_rank = model.predict(test_X)\n",
    "print(preds_rank[:30])\n",
    "print(test_X_raw[:30])\n",
    "print(test_label[:30])\n",
    "\n",
    "# Save best model\n",
    "model.save_model(f\"./models/catboost_{time}.cbm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F    \n",
    "from torch import optim \n",
    "import torchvision.models\n",
    "# from torchvision.models import resnet18\n",
    "import torchvision\n",
    "    \n",
    "    \n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "#custom\n",
    "from dataloader import CustomDataset\n",
    "\n",
    "\n",
    "class _Loss(nn.Module): \n",
    "    reduction: str\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction: str = nn._Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "            \n",
    "            \n",
    "class CustomLoss(_Loss): # 등수(label)가 3~6등일때는 비중을 줄인다. 왜? 극의 등수보단, 중간에 위치한 등수는 변동되기 쉬움을 감안하여 비중을 줄여보는 시도를 했다.\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(CustomLoss, self).__init__(size_average, reduce, reduction)\n",
    "    # def custom_loss(input, target, reduction):\n",
    "    #     return \n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        res = abs(input-target)\n",
    "        for i in res:\n",
    "            if res[0] in [3,4,5,6]:\n",
    "                res[0] *= 0.8\n",
    "        res = res*res\n",
    "        # print(res.shape)\n",
    "        return res.mean() #sum(res)/len(target)\n",
    "        return custom_loss(input, target, reduction=self.reduction)\n",
    "    \n",
    "class Net_(nn.Module): # NN, ReLU, 146 -> 64 -> 16 -> 4 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "        self.fc4 = nn.Linear(4, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class Net_2(nn.Module): # NN, ReLU, 146 -> 128 -> 64 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 4)\n",
    "        self.fc7 = nn.Linear(4, 2)\n",
    "        self.fc8 = nn.Linear(2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "    # def __init__(self) -> None:\n",
    "    #     super().__init__()\n",
    "    #     dim = 128\n",
    "    #     self.fcLayers = [nn.Linear(146, dim)]\n",
    "    #     div = 2\n",
    "    #     out_ = dim\n",
    "    #     while out_ > 1:\n",
    "    #         in_ = out_\n",
    "    #         out_ = in_//div\n",
    "    #         self.fcLayers += [nn.Linear(in_, out_)]\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     # print(\"q,\", x.shape)\n",
    "    #     for fc in self.fcLayers[:-1]:\n",
    "    #         x  = fc(x)\n",
    "    #         x = F.relu(x)\n",
    "    #     x = self.fcLayers[-1](x)\n",
    "    #     return x\n",
    "    \n",
    "class Net_3(nn.Module): # NN, ReLU, 146 -> 128 -> 112 -> 96 -> 80 -> 64 -> 48 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 128)\n",
    "        self.fc2 = nn.Linear(128, 112)\n",
    "        self.fc3 = nn.Linear(112, 96)\n",
    "        self.fc4 = nn.Linear(96, 80)\n",
    "        self.fc5 = nn.Linear(80, 64)\n",
    "        self.fc6 = nn.Linear(64, 48)\n",
    "        self.fc7 = nn.Linear(48, 32)\n",
    "        self.fc8 = nn.Linear(32, 16)\n",
    "        self.fc9 = nn.Linear(16, 8)\n",
    "        self.fc10 = nn.Linear(8, 4)\n",
    "        self.fc11 = nn.Linear(4, 2)\n",
    "        self.fc12 = nn.Linear(2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc9(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc10(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc11(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc12(x)\n",
    "        return x\n",
    "    \n",
    "class Net_4(nn.Module): # NN, ReLU, 146 -> 128 -> 96 -> 64 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 128)\n",
    "        self.fc2 = nn.Linear(128, 96)\n",
    "        self.fc3 = nn.Linear(96, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.fc6 = nn.Linear(16, 8)\n",
    "        self.fc7 = nn.Linear(8, 4)\n",
    "        self.fc8 = nn.Linear(4, 2)\n",
    "        self.fc9 = nn.Linear(2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc9(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Net_5(nn.Module): # NN, LeakyReLU, 146 -> 128 -> 64 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 4)\n",
    "        self.fc7 = nn.Linear(4, 2)\n",
    "        self.fc8 = nn.Linear(2, 1)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "\n",
    "class Net2(nn.Module): # ConvNet, Conv를 하려면 dataloader에서 추가로 수정해야한다. dataloader의 __getitem__함수에 주석되어있는 expand를 통해 강제로 크기를 늘려서 convolution을 진행한다.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=146, out_channels=256, \n",
    "                            kernel_size=3, stride=1, padding='same') \n",
    "        # in_channels, out_channels, kernel_size,stride=1, padding=0, \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding='same')\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3, 1, padding='same')\n",
    "        self.conv4 = nn.Conv2d(512, 512, 3, 1, padding='same')\n",
    "        self.conv5 = nn.Conv2d(512, 512, 3, 1, padding='same')\n",
    "        self.conv6 = nn.Conv2d(512, 1024, 3, 1, padding='same')\n",
    "        self.dropout = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"shape1:\", x.shape) # shape: torch.Size([64, 146, 4, 4])\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # print(\"shape2:\", x.shape)\n",
    "        x = torch.flatten(x,1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # print(\"shape3:\", x.shape) # shape: torch.Size([64, 8])\n",
    "        # print(\"n\", x)\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        # print(\"s\",x)\n",
    "        return x\n",
    "    \n",
    "class Net5(nn.Module): # ConvNet, Conv를 하려면 dataloader에서 추가로 수정해야한다. dataloader의 __getitem__함수에 주석되어있는 expand를 통해 강제로 크기를 늘려서 convolution을 진행한다.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=146, out_channels=256, \n",
    "                            kernel_size=3, stride=1, padding='same') \n",
    "        # in_channels, out_channels, kernel_size,stride=1, padding=0, \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding='same')\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3, 1, padding='same')\n",
    "        self.conv4 = nn.Conv2d(512, 512, 3, 1, padding='same')\n",
    "        self.conv5 = nn.Conv2d(512, 512, 3, 1, padding='same')\n",
    "        self.conv6 = nn.Conv2d(512, 1024, 3, 1, padding='same')\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, 3, 1, padding='same')\n",
    "        self.conv8 = nn.Conv2d(1024, 2048, 3, 1, padding='same')\n",
    "        self.dropout = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.fc0 = nn.Linear(2048, 1024)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"shape1:\", x.shape) # shape: torch.Size([64, 146, 4, 4])\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # print(\"shape2:\", x.shape)\n",
    "        x = torch.flatten(x,1)\n",
    "        \n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # print(\"shape3:\", x.shape) # shape: torch.Size([64, 8])\n",
    "        # print(\"n\", x)\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        # print(\"s\",x)\n",
    "        return x\n",
    "\n",
    "def train_(model, cost, optim, train_dl, device): # train_dataset을 학습시키는 함수\n",
    "    N = len(train_dl.dataset)\n",
    "    n_batch = int(N / train_dl.batch_size)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (x,y) in enumerate(train_dl):\n",
    "        # print(f\"x: {x} type: {type(x)}\\ny: {y} \\n\") # 주석처리한 print함수들은 디버깅, 테스트용 코드입니다.\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        # print(x.shape)\n",
    "        # print(\"test() x.shape\", x.shape)\n",
    "        pred = model(x) # 현재 상태의 model에 input을 넣어 prediction값을 구한다.\n",
    "        # print(\"a\")\n",
    "        # print(\"pred:\",pred.shape)\n",
    "        # print(y.shape) #torch.Size([64, 1])\n",
    "        # print(1,y[0]) #1 tensor([0.], device='cuda:0')\n",
    "        # print(2,y[0][0]) #2 tensor(0., device='cuda:0')\n",
    "        # print(3,y[0][0]==0) #3 tensor(True, device='cuda:0')\n",
    "        # print(3,y[0][0] in [0,1,2])\n",
    "        # print(y[0][0]==torch.Tensor([0.]))\n",
    "        \n",
    "        cost_ = cost(pred, y) # cost function을 통해 loss값을 구한다. \n",
    "        losses.append(cost_.item()) # \n",
    "        # Backpropagation.\n",
    "        optim.zero_grad()\n",
    "        cost_.backward() # 역전파 계산  \n",
    "        optim.step() # 최적화 \n",
    "        if batch%1000==0:\n",
    "            print(f\"\\rTrain: {batch+1}/{n_batch}\\tloss:{cost_}\")\n",
    "            # sys.stdout.write(f\"\\rTrain: {batch+1}/{n_batch} loss:{cost_}\")\n",
    "            # sys.stdout.flush()\n",
    "        # break\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "def test_(model, loss_fn, test_dl, device):\n",
    "    model = model.to(device)\n",
    "    N = len(test_dl.dataset)\n",
    "    n_batch = int(N / test_dl.batch_size)\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # test_ 에선 gradient계산이 필요하지 않으므로 불필요한 연산을 하지 않기위해 no_grad()처리를 해준다.\n",
    "        for batch, (x,y) in enumerate(test_dl):\n",
    "            x,y = x.to(device).float(), y.to(device).float()\n",
    "            \n",
    "            pred = model(x) \n",
    "            loss = loss_fn(pred, y) # cost function을 통해 loss값을 구한다. \n",
    "            losses.append(loss.item())\n",
    "            if batch%500==0:\n",
    "                print(f\"\\rTest: {batch+1}/{n_batch}\\tloss_test:{loss}\")\n",
    "                # sys.stdout.write(f\"\\rTest: {batch+1}/{n_batch} loss_test:{loss}\")\n",
    "                # sys.stdout.flush()\n",
    "            # break\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "def predict_(model, loss_fn, test_dl, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # predict_ 에선 gradient계산이 필요하지 않으므로 불필요한 연산을 하지 않기위해 no_grad()처리를 해준다.\n",
    "        for batch, (x,y) in enumerate(test_dl):\n",
    "            x,y = x.to(device).float(), y.to(device).float()\n",
    "            \n",
    "            pred = model(x)\n",
    "            L = loss_fn(pred, y)\n",
    "            y = y.cpu().numpy().squeeze()\n",
    "            pred = pred.cpu().numpy().squeeze()\n",
    "            y += 1\n",
    "            pred += 1 \n",
    "            # print(f\"y: {y[:10]}\")\n",
    "            # print(f\"h: {pred[:10]}\")\n",
    "            # print(sum(abs(y[:10]-pred[:10])))\n",
    "            # print(sum(abs(y-pred)),'\\n\\n')\n",
    "            # quit()\n",
    "        # L1 loss와 L2 loss 둘 다 판단하는게 필요한 task였기 때문에 predict_함수에서 두 loss를 계산했다.\n",
    "        # L1과 L2 loss 둘 다 비교하는 이유는 두 loss가 이번 task에서 가지는 특성이 있기 때문이다.\n",
    "        # L1은 예측 등수가 크게 차이나도 linear한 증가치로 loss를 측정하고,\n",
    "        # L2는 등수 예측 차이가 1 이하로 나면 (L1에 비해) 상대적으로 적은 loss를, 예측 차이가 1 이상으로(크게)나면 상대적으로 더 큰 loss를 준다.\n",
    "        # 이는 label이 '등수'라는 상황에선 중요하게 고려해야할 요소라고 판단했다.\n",
    "        print(f\"L1 loss: {sum(abs(y-pred))/len(y)}\") \n",
    "        print(f\"L2 loss: {L}\")\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = CustomDataset()\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset, [0.8, 0.2]) # 데이터셋을 8 : 2비율로 나눠준다. \n",
    "    \n",
    "    # model = Net2().to(device)\n",
    "    # model = resnet18(num_classes=1)\n",
    "    # model = resnet18(num_classes=1).to(device)\n",
    "    # model = resnet50(num_classes=1).to(device)\n",
    "    # model = Net_4().to(device)\n",
    "    model = Net_2().to(device) \n",
    "    # 최종적으로 채택한 모델은 Net_2()이다. convolution layer를 추가한 모델도 이와 유사한 성능을 보였으나, 큰 차이가 나지 않았고 computation관점에서 생각을 했을 때 Net_2()가 더 낫다고 생각했다.\n",
    "    \n",
    "    criterion = nn.MSELoss().to(device) # loss = Mean Square Error를 사용했다.\n",
    "    # criterion = CustomLoss().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    \n",
    "    epochs = 30\n",
    "    load_name = \"model_param14_justDNN25.pt\" # 이미 train시킨 모델의 파라미터를 불러오려면 이 변수 이름을 변경하여 사용한다.\n",
    "    # model_param12_justDNN23 net2, lr=0.0005, ep=30 bset,,\n",
    "    # model_param13_justDNN24 net2, lr=0.0003, ep=40\n",
    "    # model_param14_justDNN25 net2, lr=0.0007, ep=30\n",
    "    train_dl = DataLoader(train_ds, \n",
    "                            batch_size=64, \n",
    "                            shuffle=True,\n",
    "                            drop_last=True\n",
    "                        ) # train dataset을 dataloader에 담아준다. \n",
    "    test_dl  = DataLoader(test_ds, \n",
    "                            batch_size=64, \n",
    "                            shuffle=True,\n",
    "                            drop_last=True\n",
    "                        ) # test datasert을 dataloader에 담아준다. \n",
    "    try: # load_name에 맞는 파라미터 저장 정보가 있으면 load해서 성능 측정(predict_)하고, 저장 정보가 없으면 except구문을 실행한다(== train시작).\n",
    "        # model.load_state_dict(torch.load(\"model_param6_custom_loss.pt\", map_location=device))\n",
    "        model.load_state_dict(torch.load(load_name, map_location=device))\n",
    "        predict_(model, criterion, test_dl, device)\n",
    "        print(1)\n",
    "    except:\n",
    "        # print(torchvision.models.list_models())\n",
    "        # transform = transforms.Compose([\n",
    "        #         # transforms.ToTensor(),\n",
    "        #         transforms.Resize(224),\n",
    "        #         # transforms.RandomHorizontalFlip(),\n",
    "        #     ])\n",
    "        \n",
    "        # print(f\"len: {len(train_ds)}\\nlen: {len(test_ds)}\") #len: 378796, len: 94698\n",
    "        \n",
    "        # model = nn.Sequential(nn.Linear(146, 128),\n",
    "        #                       nn.ReLU(),\n",
    "        #                       nn.Linear(128, 64),\n",
    "        #                       nn.ReLU(),\n",
    "        #                       nn.Linear(64, 8)\n",
    "        #                        )\n",
    "        # model = model.to(device)\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        min_ = 999\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"[ epoch: {epoch+1}/{epochs} ]\")\n",
    "            train_loss.append(train_(model, criterion, optimizer, train_dl, device)) # training & train loss를 list에 저장한다. -> plot을 하기 위한 용도\n",
    "            temp = test_(model, criterion, test_dl, device) # testing\n",
    "            test_loss.append(temp) # test loss를 list에 저장한다. -> plot을 하기 위한 용도\n",
    "            if min_ > temp:  # 가장 loss가 적을때의 파라미터 정보를 저장한다.\n",
    "                min_ = temp\n",
    "                torch.save(model.state_dict(), load_name) # 파라미터 정보를 저장하는 코드이다. \n",
    "        # torch.save(model.state_dict(), load_name)\n",
    "        print(train_loss)\n",
    "        print(test_loss)\n",
    "        print(min_)\n",
    "        plt.plot(train_loss, 'b-', label='train_loss') # train_loss plotting\n",
    "        plt.plot(test_loss, 'r-', label='test_loss')   # test_loss plotting\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{load_name}.png\") # 사진 저장\n",
    "        predict_(model, criterion, test_dl, device)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
