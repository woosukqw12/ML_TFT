{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tft_data.py\n",
    "\n",
    "mergedata.py, \n",
    "\n",
    "metaGen_ordenc.py, \n",
    "\n",
    "metaList_ordenc.txt\n",
    "\n",
    "encoder.py,\n",
    "\n",
    "metaGenerator.py, (data/)\n",
    "\n",
    "model_catboost.py\n",
    "\n",
    "dataloader.py\n",
    "\n",
    "model_woo2.py\n",
    "\n",
    "Data_processing_Fixed.py \n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "\n",
    "샘플데이터(match_data1.pickle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import category_encoders as ce\n",
    "import math\n",
    "\n",
    "# rarity - cost array\n",
    "# e.g. Ao Shin is Rarity 7, 10 cost\n",
    "cost = [0, 1, 2, 3, 4, 8, 5, 10]\n",
    "pw = [\n",
    "    [0, 1, 2, 3, 4, 8, 5, 10],\n",
    "    [\n",
    "        0,\n",
    "        3,\n",
    "        5,\n",
    "    ],\n",
    "]\n",
    "\n",
    "\n",
    "def multi_hot_encoding(df):\n",
    "    ## Feature list import\n",
    "    MetaData = []\n",
    "    # open metadata for multi hot encoding\n",
    "    with open(\"./data/metaList4.txt\", \"r\") as a:\n",
    "        s = a.readlines()\n",
    "    for line in s:\n",
    "        res = re.sub(\"'|,| |\\n\", \"\", line)\n",
    "        MetaData.append(res)\n",
    "\n",
    "    X = []\n",
    "    raw_X = []\n",
    "    label = []\n",
    "    for game in df[\"info.participants\"]:\n",
    "        # game is list of 8 players\n",
    "        # j has augments, companion, gold_left, last_round, level, placement, players_eliminated, puuid, traits, units\n",
    "        # j is single player\n",
    "        for j in game:\n",
    "            value = 0\n",
    "            X_i = []\n",
    "            # ignore if died before 4-2 (outlier)\n",
    "            if len(j[\"augments\"]) < 3:\n",
    "                continue\n",
    "            # trait tier_current is activated number of synergy (e.g. 석호6)\n",
    "            for trait in j[\"traits\"]:\n",
    "                if trait[\"name\"] + str(trait[\"tier_current\"]) in MetaData:\n",
    "                    X_i.append(trait[\"name\"] + str(trait[\"num_units\"]))\n",
    "            for champ in j[\"units\"]:\n",
    "                value += cost[champ[\"rarity\"]] * pow(3, champ[\"tier\"] - 1)\n",
    "                if champ[\"itemNames\"] != []:\n",
    "                    if champ[\"itemNames\"][0] == \"TFT_Item_ThiefsGloves\":\n",
    "                        X_i.append(\"TFT_Item_ThiefsGloves\")\n",
    "                    else:\n",
    "                        for item in champ[\"itemNames\"]:\n",
    "                            if item in MetaData:\n",
    "                                X_i.append(item)\n",
    "            for aug in j[\"augments\"]:\n",
    "                if aug in MetaData:\n",
    "                    X_i.append(aug)\n",
    "\n",
    "            X_i.append(value)  # total cost of deck\n",
    "            X_i.append(j[\"total_damage_to_players\"])  # total damage to players\n",
    "            raw_X.append(j)\n",
    "            label.append(j[\"placement\"])  # rank (label)\n",
    "            X.append(sorted(X_i))  # sorted\n",
    "\n",
    "    X = [[1 if i in row_x else 0 for i in MetaData] for row_x in X]\n",
    "    # multi-hot_encoding\n",
    "    return X, label, raw_X\n",
    "\n",
    "\n",
    "def ordered_target_encoding(df):\n",
    "    ## Feature list import\n",
    "    MetaData = []\n",
    "    with open(\"./data/metaList_ordenc.txt\", \"r\") as a:\n",
    "        s = a.readlines()\n",
    "    for line in s:\n",
    "        res = re.sub(\"'|,| |\\n\", \"\", line)\n",
    "        MetaData.append(res)\n",
    "\n",
    "    # print(MetaData)\n",
    "    X = []\n",
    "    raw_X = []\n",
    "    label = []\n",
    "    train_aug1 = []\n",
    "    train_aug2 = []\n",
    "    train_aug3 = []\n",
    "    train_aug = {}\n",
    "    # depenency category_encoders\n",
    "    cbe_aug = ce.CatBoostEncoder()\n",
    "    for game in df[\"info.participants\"]:\n",
    "        # game is list of 8 players\n",
    "        # j has augments, companion, gold_left, last_round, level, placement, players_eliminated, puuid, traits, units\n",
    "        # j is single player\n",
    "        for j in game:\n",
    "            # item (112) + trait (29) + aug (3) + special feature (2) = 146\n",
    "            value = 0\n",
    "            X_i = [0 for i in range(141)]\n",
    "            # ignore if died before 4-2 (outlier)\n",
    "            if len(j[\"augments\"]) < 3:\n",
    "                continue\n",
    "            for champ in j[\"units\"]:\n",
    "                value += cost[champ[\"rarity\"]] * pow(3, champ[\"tier\"] - 1)\n",
    "                if champ[\"itemNames\"] != []:\n",
    "                    if champ[\"itemNames\"][0] == \"TFT_Item_ThiefsGloves\":\n",
    "                        idx = MetaData.index(\"TFT_Item_ThiefsGloves\")\n",
    "                        X_i[idx] = 1\n",
    "                    else:\n",
    "                        for item in champ[\"itemNames\"]:\n",
    "                            if item in MetaData:\n",
    "                                idx = MetaData.index(item)\n",
    "                                X_i[idx] = 1\n",
    "\n",
    "            # trait tier_current is activated number of synergy (e.g. 석호6)\n",
    "            for trait in j[\"traits\"]:\n",
    "                if trait[\"name\"] in MetaData:\n",
    "                    idx = MetaData.index(trait[\"name\"])\n",
    "                    # print(\"trait index is \" + str(idx))\n",
    "                    # print(trait[\"tier_current\"])\n",
    "                    X_i[idx] = math.exp(trait[\"num_units\"])\n",
    "            train_aug1.append(j[\"augments\"][0])\n",
    "            train_aug2.append(j[\"augments\"][1])\n",
    "            train_aug3.append(j[\"augments\"][2])\n",
    "            X_i.append(value)  # total cost of deck\n",
    "            X_i.append(j[\"total_damage_to_players\"])  # total damage to players\n",
    "            label.append(j[\"placement\"])  # rank (label)\n",
    "            X.append(X_i)\n",
    "            raw_X.append(j)\n",
    "\n",
    "    train_aug[\"aug1\"] = train_aug1\n",
    "    train_aug[\"aug2\"] = train_aug2\n",
    "    train_aug[\"aug3\"] = train_aug3\n",
    "    print(\"Start cbe\")\n",
    "    print(len(train_aug1), len(train_aug2), len(train_aug3), len(label))\n",
    "    train_aug_df = pd.DataFrame(train_aug)\n",
    "\n",
    "    # X of cbe_aug should be pd.DataFrame\n",
    "    # Ordered Target Encoding\n",
    "    cbe_aug.fit(train_aug_df, label)\n",
    "    aug_cbe = cbe_aug.transform(train_aug_df)\n",
    "    for i in range(len(X)):\n",
    "        # print(aug_cbe[\"aug1\"][i], aug_cbe[\"aug2\"][i], aug_cbe[\"aug3\"][i])\n",
    "        X[i].append(aug_cbe[\"aug1\"][i])\n",
    "        X[i].append(aug_cbe[\"aug2\"][i])\n",
    "        X[i].append(aug_cbe[\"aug3\"][i])\n",
    "        assert len(X[i]) == 146\n",
    "    return X, label, raw_X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import category_encoders as ce\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, transform=None):\n",
    "    #   데이터셋의 전처리를 해주는 부분\n",
    "        self.transform = transform\n",
    "        ## Feature list import\n",
    "        df = pd.read_pickle(\"./data/match_data/match_data1.pickle\")\n",
    "        # delete 404 and pairs & turbo mode\n",
    "        df = df[df[\"status.status_code\"].isna()]\n",
    "        df = df[df[\"info.tft_game_type\"] == \"standard\"]\n",
    "        \n",
    "        MetaData = []\n",
    "        cost = [0, 1, 2, 3, 4, 8, 5, 10]\n",
    "        with open(\"./data/metaList_ordenc.txt\", \"r\") as a:\n",
    "            s = a.readlines()\n",
    "        for line in s:\n",
    "            res = re.sub(\"'|,| |\\n\", \"\", line)\n",
    "            MetaData.append(res)\n",
    "\n",
    "        # print(MetaData)\n",
    "        self.X = []\n",
    "        raw_X = []\n",
    "        self.label = []\n",
    "        train_aug1 = []\n",
    "        train_aug2 = []\n",
    "        train_aug3 = []\n",
    "        train_aug = {}\n",
    "        # depenency category_encoders\n",
    "        cbe_aug = ce.CatBoostEncoder()\n",
    "        for game in df[\"info.participants\"]:\n",
    "            # game is list of 8 players\n",
    "            # j has augments, companion, gold_left, last_round, level, placement, players_eliminated, puuid, traits, units\n",
    "            # j is single player\n",
    "            for j in game:\n",
    "                # item (112) + trait (29) + aug (3) + value (2) = 146\n",
    "                value = 0\n",
    "                X_i = [0 for i in range(141)]\n",
    "                # ignore if died before 4-2\n",
    "                if len(j[\"augments\"]) < 3:\n",
    "                    continue\n",
    "                for champ in j[\"units\"]:\n",
    "                    value += cost[champ[\"rarity\"]] * pow(3, champ[\"tier\"] - 1)\n",
    "                    if champ[\"itemNames\"] != []:\n",
    "                        if champ[\"itemNames\"][0] == \"TFT_Item_ThiefsGloves\":\n",
    "                            idx = MetaData.index(\"TFT_Item_ThiefsGloves\")\n",
    "                            X_i[idx] = 1\n",
    "                        else:\n",
    "                            for item in champ[\"itemNames\"]:\n",
    "                                if item in MetaData:\n",
    "                                    idx = MetaData.index(item)\n",
    "                                    X_i[idx] = 1\n",
    "\n",
    "                # trait tier_current is activated number of synergy (e.g. 석호6)\n",
    "                for trait in j[\"traits\"]:\n",
    "                    if trait[\"name\"] in MetaData:\n",
    "                        idx = MetaData.index(trait[\"name\"])\n",
    "                        # print(\"trait index is \" + str(idx))\n",
    "                        # print(trait[\"tier_current\"])\n",
    "                        X_i[idx] = math.exp(trait[\"num_units\"]) / 4\n",
    "                train_aug1.append(j[\"augments\"][0])\n",
    "                train_aug2.append(j[\"augments\"][1])\n",
    "                train_aug3.append(j[\"augments\"][2])\n",
    "                X_i.append(value)  # total cost of deck\n",
    "                X_i.append(j[\"total_damage_to_players\"])  # total damage to players\n",
    "                self.label.append(j[\"placement\"])\n",
    "                self.X.append(X_i)\n",
    "                raw_X.append(j)\n",
    "\n",
    "        train_aug[\"aug1\"] = train_aug1\n",
    "        train_aug[\"aug2\"] = train_aug2\n",
    "        train_aug[\"aug3\"] = train_aug3\n",
    "        # print(\"Start cbe\")\n",
    "        # print(len(train_aug1), len(train_aug2), len(train_aug3), len(label))\n",
    "        train_aug_df = pd.DataFrame(train_aug)\n",
    "\n",
    "        # X of cbe_aug should be pd.DataFrame\n",
    "        cbe_aug.fit(train_aug_df, self.label)\n",
    "        aug_cbe = cbe_aug.transform(train_aug_df)\n",
    "        for i in range(len(self.X)):\n",
    "            # print(aug_cbe[\"aug1\"][i], aug_cbe[\"aug2\"][i], aug_cbe[\"aug3\"][i])\n",
    "            self.X[i].append(aug_cbe[\"aug1\"][i])\n",
    "            self.X[i].append(aug_cbe[\"aug2\"][i])\n",
    "            self.X[i].append(aug_cbe[\"aug3\"][i])\n",
    "            assert len(self.X[i]) == 146\n",
    "        # return X, label, raw_X\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #   데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "        return len(self.X)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        X_ = self.X[idx]\n",
    "        if self.transform:\n",
    "            X_ = self.transform(X_)\n",
    "        #   데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "        # X_ = np.expand_dims(X_, axis=(1,2)) # just DNN에선 지우기\n",
    "        X_ = torch.tensor(X_).to(torch.float32)\n",
    "        # X_ = X_.expand(-1, 4, 4)\n",
    "        # (146, 1, 1) -> (146, 4, 4) !!!!! 인코딩 값 146 시너지 + 아이템 + 증강체 + 커스텀 밸류 \n",
    "        # X_ = X_.expand(-1, 8, 8)\n",
    "        # X_ = X_.expand(-1, 16, 16)\n",
    "        \n",
    "        label_ = torch.tensor(self.label[idx]-1).to(torch.float32)\n",
    "        # label_ = label_.view(1)\n",
    "        label_ = label_.unsqueeze(0)\n",
    "        # print(label_.shape,\"Wdqw\")\n",
    "        # print(X_.shape)\n",
    "        # print(\"asdasd\", X_i.shape)\n",
    "        \n",
    "        # print(\"asdasd\", X_i.shape)\n",
    "        return X_, label_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X):\n",
    "    l = len(X)\n",
    "    train = X[0 : l // 2]\n",
    "    valid = X[l // 2 : l * 3 // 4]\n",
    "    test = X[l * 3 // 4 : l]\n",
    "    assert l == len(train) + len(valid) + len(test)\n",
    "    return train, valid, test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://catboost.ai/en/docs/concepts/python-reference_catboost\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
    "import pandas as pd\n",
    "from encoder import ordered_target_encoding, multi_hot_encoding\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "time = datetime.now(pytz.timezone(\"Asia/Seoul\")).strftime(\"%Y%m%d_%H%M%S\")\n",
    "# read data from merged pickle\n",
    "# df = pd.read_pickle(\"./data/match_data/match_data.pickle\") \n",
    "# 이 파일(match_data.pickle)이 match_data1~30의 내용이 모두 merge된 파일이며, 본 러닝에서 사용했습니다. 그러나 용량 문제로 데모 데이터셋(match_data1.pickle)을 대신 제출했습니다. \n",
    "df = pd.read_pickle(\"./data/match_data/match_data1.pickle\")\n",
    "\n",
    "# delete 404 Not Found and pairs & turbo mode\n",
    "df = df[df[\"status.status_code\"].isna()]\n",
    "df = df[df[\"info.tft_game_type\"] == \"standard\"]\n",
    "print(len(df))\n",
    "\n",
    "# get ordered target encoded categorical features\n",
    "X, label, X_raw = ordered_target_encoding(df)\n",
    "assert len(X) == len(label)\n",
    "\n",
    "# split dataset into train, valid, test\n",
    "train_X, valid_X, test_X = split_data(X)\n",
    "train_label, valid_label, test_label = split_data(label)\n",
    "train_X_raw, valid_X_raw, test_X_raw = split_data(X_raw)\n",
    "\n",
    "\n",
    "train_data = Pool(data=train_X, label=train_label)\n",
    "valid_data = Pool(data=valid_X, label=valid_label)\n",
    "test_data = Pool(data=test_X, label=test_label)\n",
    "# default lr = 0.03\n",
    "# CatBoost Regression Tree\n",
    "model = CatBoostRegressor(iterations=5000, task_type=\"GPU\")\n",
    "\n",
    "print(\"Train Start!\")\n",
    "\n",
    "# Train\n",
    "model.fit(train_data, eval_set=valid_data, plot=True)\n",
    "print(model.get_best_iteration())\n",
    "print(model.get_best_score())\n",
    "\n",
    "# Inference\n",
    "preds_rank = model.predict(test_X)\n",
    "print(preds_rank[:30])\n",
    "print(test_X_raw[:30])\n",
    "print(test_label[:30])\n",
    "\n",
    "# Save best model\n",
    "model.save_model(f\"./models/catboost_{time}.cbm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss: 0.5715732630342245\n",
      "L2 loss: 0.5502856373786926\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F    \n",
    "from torch import optim \n",
    "import torchvision.models\n",
    "# from torchvision.models import resnet18\n",
    "import torchvision\n",
    "    \n",
    "    \n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "#custom\n",
    "from dataloader import CustomDataset\n",
    "\n",
    "\n",
    "class _Loss(nn.Module): \n",
    "    reduction: str\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction: str = nn._Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "            \n",
    "            \n",
    "class CustomLoss(_Loss): # 등수(label)가 3~6등일때는 비중을 줄인다. 왜? 극의 등수보단, 중간에 위치한 등수는 변동되기 쉬움을 감안하여 비중을 줄여보는 시도를 했다.\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(CustomLoss, self).__init__(size_average, reduce, reduction)\n",
    "    # def custom_loss(input, target, reduction):\n",
    "    #     return \n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        res = abs(input-target)\n",
    "        for i in res:\n",
    "            if res[0] in [3,4,5,6]:\n",
    "                res[0] *= 0.8\n",
    "        res = res*res\n",
    "        # print(res.shape)\n",
    "        return res.mean() #sum(res)/len(target)\n",
    "        return custom_loss(input, target, reduction=self.reduction)\n",
    "    \n",
    "class Net_(nn.Module): # NN, ReLU, 146 -> 64 -> 16 -> 4 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "        self.fc4 = nn.Linear(4, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class Net_2(nn.Module): # NN, ReLU, 146 -> 128 -> 64 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 4)\n",
    "        self.fc7 = nn.Linear(4, 2)\n",
    "        self.fc8 = nn.Linear(2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "    # def __init__(self) -> None:\n",
    "    #     super().__init__()\n",
    "    #     dim = 128\n",
    "    #     self.fcLayers = [nn.Linear(146, dim)]\n",
    "    #     div = 2\n",
    "    #     out_ = dim\n",
    "    #     while out_ > 1:\n",
    "    #         in_ = out_\n",
    "    #         out_ = in_//div\n",
    "    #         self.fcLayers += [nn.Linear(in_, out_)]\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     # print(\"q,\", x.shape)\n",
    "    #     for fc in self.fcLayers[:-1]:\n",
    "    #         x  = fc(x)\n",
    "    #         x = F.relu(x)\n",
    "    #     x = self.fcLayers[-1](x)\n",
    "    #     return x\n",
    "    \n",
    "class Net_3(nn.Module): # NN, ReLU, 146 -> 128 -> 112 -> 96 -> 80 -> 64 -> 48 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 128)\n",
    "        self.fc2 = nn.Linear(128, 112)\n",
    "        self.fc3 = nn.Linear(112, 96)\n",
    "        self.fc4 = nn.Linear(96, 80)\n",
    "        self.fc5 = nn.Linear(80, 64)\n",
    "        self.fc6 = nn.Linear(64, 48)\n",
    "        self.fc7 = nn.Linear(48, 32)\n",
    "        self.fc8 = nn.Linear(32, 16)\n",
    "        self.fc9 = nn.Linear(16, 8)\n",
    "        self.fc10 = nn.Linear(8, 4)\n",
    "        self.fc11 = nn.Linear(4, 2)\n",
    "        self.fc12 = nn.Linear(2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc9(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc10(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc11(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc12(x)\n",
    "        return x\n",
    "    \n",
    "class Net_4(nn.Module): # NN, ReLU, 146 -> 128 -> 96 -> 64 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 128)\n",
    "        self.fc2 = nn.Linear(128, 96)\n",
    "        self.fc3 = nn.Linear(96, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.fc6 = nn.Linear(16, 8)\n",
    "        self.fc7 = nn.Linear(8, 4)\n",
    "        self.fc8 = nn.Linear(4, 2)\n",
    "        self.fc9 = nn.Linear(2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc9(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Net_5(nn.Module): # NN, LeakyReLU, 146 -> 128 -> 64 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(146, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 4)\n",
    "        self.fc7 = nn.Linear(4, 2)\n",
    "        self.fc8 = nn.Linear(2, 1)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"q,\", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.leaky(x)\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "\n",
    "class Net2(nn.Module): # ConvNet, Conv를 하려면 dataloader에서 추가로 수정해야한다. dataloader의 __getitem__함수에 주석되어있는 expand를 통해 강제로 크기를 늘려서 convolution을 진행한다.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=146, out_channels=256, \n",
    "                            kernel_size=3, stride=1, padding='same') \n",
    "        # in_channels, out_channels, kernel_size,stride=1, padding=0, \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding='same')\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3, 1, padding='same')\n",
    "        self.conv4 = nn.Conv2d(512, 512, 3, 1, padding='same')\n",
    "        self.conv5 = nn.Conv2d(512, 512, 3, 1, padding='same')\n",
    "        self.conv6 = nn.Conv2d(512, 1024, 3, 1, padding='same')\n",
    "        self.dropout = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"shape1:\", x.shape) # shape: torch.Size([64, 146, 4, 4])\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # print(\"shape2:\", x.shape)\n",
    "        x = torch.flatten(x,1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # print(\"shape3:\", x.shape) # shape: torch.Size([64, 8])\n",
    "        # print(\"n\", x)\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        # print(\"s\",x)\n",
    "        return x\n",
    "    \n",
    "class Net5(nn.Module): # ConvNet, Conv를 하려면 dataloader에서 추가로 수정해야한다. dataloader의 __getitem__함수에 주석되어있는 expand를 통해 강제로 크기를 늘려서 convolution을 진행한다.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=146, out_channels=256, \n",
    "                            kernel_size=3, stride=1, padding='same') \n",
    "        # in_channels, out_channels, kernel_size,stride=1, padding=0, \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding='same')\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3, 1, padding='same')\n",
    "        self.conv4 = nn.Conv2d(512, 512, 3, 1, padding='same')\n",
    "        self.conv5 = nn.Conv2d(512, 512, 3, 1, padding='same')\n",
    "        self.conv6 = nn.Conv2d(512, 1024, 3, 1, padding='same')\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, 3, 1, padding='same')\n",
    "        self.conv8 = nn.Conv2d(1024, 2048, 3, 1, padding='same')\n",
    "        self.dropout = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.fc0 = nn.Linear(2048, 1024)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"shape1:\", x.shape) # shape: torch.Size([64, 146, 4, 4])\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # print(\"shape2:\", x.shape)\n",
    "        x = torch.flatten(x,1)\n",
    "        \n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # print(\"shape3:\", x.shape) # shape: torch.Size([64, 8])\n",
    "        # print(\"n\", x)\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        # print(\"s\",x)\n",
    "        return x\n",
    "\n",
    "def train_(model, cost, optim, train_dl, device): # train_dataset을 학습시키는 함수\n",
    "    N = len(train_dl.dataset)\n",
    "    n_batch = int(N / train_dl.batch_size)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (x,y) in enumerate(train_dl):\n",
    "        # print(f\"x: {x} type: {type(x)}\\ny: {y} \\n\") # 주석처리한 print함수들은 디버깅, 테스트용 코드입니다.\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        # print(x.shape)\n",
    "        # print(\"test() x.shape\", x.shape)\n",
    "        pred = model(x) # 현재 상태의 model에 input을 넣어 prediction값을 구한다.\n",
    "        # print(\"a\")\n",
    "        # print(\"pred:\",pred.shape)\n",
    "        # print(y.shape) #torch.Size([64, 1])\n",
    "        # print(1,y[0]) #1 tensor([0.], device='cuda:0')\n",
    "        # print(2,y[0][0]) #2 tensor(0., device='cuda:0')\n",
    "        # print(3,y[0][0]==0) #3 tensor(True, device='cuda:0')\n",
    "        # print(3,y[0][0] in [0,1,2])\n",
    "        # print(y[0][0]==torch.Tensor([0.]))\n",
    "        \n",
    "        cost_ = cost(pred, y) # cost function을 통해 loss값을 구한다. \n",
    "        losses.append(cost_.item()) # \n",
    "        # Backpropagation.\n",
    "        optim.zero_grad()\n",
    "        cost_.backward() # 역전파 계산  \n",
    "        optim.step() # 최적화 \n",
    "        if batch%1000==0:\n",
    "            print(f\"\\rTrain: {batch+1}/{n_batch}\\tloss:{cost_}\")\n",
    "            # sys.stdout.write(f\"\\rTrain: {batch+1}/{n_batch} loss:{cost_}\")\n",
    "            # sys.stdout.flush()\n",
    "        # break\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "def test_(model, loss_fn, test_dl, device):\n",
    "    model = model.to(device)\n",
    "    N = len(test_dl.dataset)\n",
    "    n_batch = int(N / test_dl.batch_size)\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # test_ 에선 gradient계산이 필요하지 않으므로 불필요한 연산을 하지 않기위해 no_grad()처리를 해준다.\n",
    "        for batch, (x,y) in enumerate(test_dl):\n",
    "            x,y = x.to(device).float(), y.to(device).float()\n",
    "            \n",
    "            pred = model(x) \n",
    "            loss = loss_fn(pred, y) # cost function을 통해 loss값을 구한다. \n",
    "            losses.append(loss.item())\n",
    "            if batch%500==0:\n",
    "                print(f\"\\rTest: {batch+1}/{n_batch}\\tloss_test:{loss}\")\n",
    "                # sys.stdout.write(f\"\\rTest: {batch+1}/{n_batch} loss_test:{loss}\")\n",
    "                # sys.stdout.flush()\n",
    "            # break\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "def predict_(model, loss_fn, test_dl, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # predict_ 에선 gradient계산이 필요하지 않으므로 불필요한 연산을 하지 않기위해 no_grad()처리를 해준다.\n",
    "        for batch, (x,y) in enumerate(test_dl):\n",
    "            x,y = x.to(device).float(), y.to(device).float()\n",
    "            \n",
    "            pred = model(x)\n",
    "            L = loss_fn(pred, y)\n",
    "            y = y.cpu().numpy().squeeze()\n",
    "            pred = pred.cpu().numpy().squeeze()\n",
    "            y += 1\n",
    "            pred += 1 \n",
    "            # print(f\"y: {y[:10]}\")\n",
    "            # print(f\"h: {pred[:10]}\")\n",
    "            # print(sum(abs(y[:10]-pred[:10])))\n",
    "            # print(sum(abs(y-pred)),'\\n\\n')\n",
    "            # quit()\n",
    "        # L1 loss와 L2 loss 둘 다 판단하는게 필요한 task였기 때문에 predict_함수에서 두 loss를 계산했다.\n",
    "        # L1과 L2 loss 둘 다 비교하는 이유는 두 loss가 이번 task에서 가지는 특성이 있기 때문이다.\n",
    "        # L1은 예측 등수가 크게 차이나도 linear한 증가치로 loss를 측정하고,\n",
    "        # L2는 등수 예측 차이가 1 이하로 나면 (L1에 비해) 상대적으로 적은 loss를, 예측 차이가 1 이상으로(크게)나면 상대적으로 더 큰 loss를 준다.\n",
    "        # 이는 label이 '등수'라는 상황에선 중요하게 고려해야할 요소라고 판단했다.\n",
    "        print(f\"L1 loss: {sum(abs(y-pred))/len(y)}\") \n",
    "        print(f\"L2 loss: {L}\")\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = CustomDataset()\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset, [0.8, 0.2]) # 데이터셋을 8 : 2비율로 나눠준다. \n",
    "    \n",
    "    # model = Net2().to(device)\n",
    "    # model = resnet18(num_classes=1)\n",
    "    # model = resnet18(num_classes=1).to(device)\n",
    "    # model = resnet50(num_classes=1).to(device)\n",
    "    # model = Net_4().to(device)\n",
    "    model = Net_2().to(device) \n",
    "    # 최종적으로 채택한 모델은 Net_2()이다. convolution layer를 추가한 모델도 이와 유사한 성능을 보였으나, 큰 차이가 나지 않았고 computation관점에서 생각을 했을 때 Net_2()가 더 낫다고 생각했다.\n",
    "    \n",
    "    criterion = nn.MSELoss().to(device) # loss = Mean Square Error를 사용했다.\n",
    "    # criterion = CustomLoss().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    \n",
    "    epochs = 30\n",
    "    load_name = \"model_param14_justDNN25.pt\" # 이미 train시킨 모델의 파라미터를 불러오려면 이 변수 이름을 변경하여 사용한다.\n",
    "    # model_param12_justDNN23 net2, lr=0.0005, ep=30 bset,,\n",
    "    # model_param13_justDNN24 net2, lr=0.0003, ep=40\n",
    "    # model_param14_justDNN25 net2, lr=0.0007, ep=30\n",
    "    train_dl = DataLoader(train_ds, \n",
    "                            batch_size=64, \n",
    "                            shuffle=True,\n",
    "                            drop_last=True\n",
    "                        ) # train dataset을 dataloader에 담아준다. \n",
    "    test_dl  = DataLoader(test_ds, \n",
    "                            batch_size=64, \n",
    "                            shuffle=True,\n",
    "                            drop_last=True\n",
    "                        ) # test datasert을 dataloader에 담아준다. \n",
    "    try: # load_name에 맞는 파라미터 저장 정보가 있으면 load해서 성능 측정(predict_)하고, 저장 정보가 없으면 except구문을 실행한다(== train시작).\n",
    "        # model.load_state_dict(torch.load(\"model_param6_custom_loss.pt\", map_location=device))\n",
    "        model.load_state_dict(torch.load(load_name, map_location=device))\n",
    "        predict_(model, criterion, test_dl, device)\n",
    "        print(1)\n",
    "    except:\n",
    "        # print(torchvision.models.list_models())\n",
    "        # transform = transforms.Compose([\n",
    "        #         # transforms.ToTensor(),\n",
    "        #         transforms.Resize(224),\n",
    "        #         # transforms.RandomHorizontalFlip(),\n",
    "        #     ])\n",
    "        \n",
    "        # print(f\"len: {len(train_ds)}\\nlen: {len(test_ds)}\") #len: 378796, len: 94698\n",
    "        \n",
    "        # model = nn.Sequential(nn.Linear(146, 128),\n",
    "        #                       nn.ReLU(),\n",
    "        #                       nn.Linear(128, 64),\n",
    "        #                       nn.ReLU(),\n",
    "        #                       nn.Linear(64, 8)\n",
    "        #                        )\n",
    "        # model = model.to(device)\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        min_ = 999\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"[ epoch: {epoch+1}/{epochs} ]\")\n",
    "            train_loss.append(train_(model, criterion, optimizer, train_dl, device)) # training & train loss를 list에 저장한다. -> plot을 하기 위한 용도\n",
    "            temp = test_(model, criterion, test_dl, device) # testing\n",
    "            test_loss.append(temp) # test loss를 list에 저장한다. -> plot을 하기 위한 용도\n",
    "            if min_ > temp:  # 가장 loss가 적을때의 파라미터 정보를 저장한다.\n",
    "                min_ = temp\n",
    "                torch.save(model.state_dict(), load_name) # 파라미터 정보를 저장하는 코드이다. \n",
    "        # torch.save(model.state_dict(), load_name)\n",
    "        print(train_loss)\n",
    "        print(test_loss)\n",
    "        print(min_)\n",
    "        plt.plot(train_loss, 'b-', label='train_loss') # train_loss plotting\n",
    "        plt.plot(test_loss, 'r-', label='test_loss')   # test_loss plotting\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{load_name}.png\") # 사진 저장\n",
    "        predict_(model, criterion, test_dl, device)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 30 2022, 05:12:36) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
